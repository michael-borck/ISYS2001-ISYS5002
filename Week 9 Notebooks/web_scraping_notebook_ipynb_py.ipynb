{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/michael-borck/ISYS2001-ISYS5002/blob/main/Week%208%20Notebooks/web_scraping_notebook_ipynb_py.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5a886VQgOugV"
   },
   "source": [
    "# Web Scraping Basics: A Friendly Introduction for Beginners\n",
    "**Learn how to collect and analyze web data using Python, Pandas, and BeautifulSoup**\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand what web scraping is and why it's useful.\n",
    "- Learn how to extract data from websites using BeautifulSoup.\n",
    "- Use pandas to organize and analyze the scraped data.\n",
    "- Practice ethical web scraping techniques.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2M7iDuvEOugW"
   },
   "source": [
    "## What is Web Scraping?\n",
    "\n",
    "Web scraping is the process of automatically collecting information from websites.  \n",
    "It helps you gather data that might not be available through APIs or downloads.\n",
    "\n",
    "We'll use a combination of libraries:\n",
    "- **requests**: to download web pages\n",
    "- **BeautifulSoup**: to parse HTML and extract information\n",
    "- **pandas**: to organize the scraped data\n",
    "\n",
    "### Installing and Importing\n",
    "\n",
    "Let's install the necessary libraries:\n",
    "\n",
    "```python\n",
    "!pip install pandas requests beautifulsoup4\n",
    "```\n",
    "\n",
    "Now let's import them:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TYyKK1yDOugX"
   },
   "outputs": [],
   "source": [
    "import pandas as pd  # For data manipulation\n",
    "import requests  # For making HTTP requests\n",
    "from bs4 import BeautifulSoup  # For parsing HTML content\n",
    "import time  # For adding delays between requests (ethical scraping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yph-c3FSOugY"
   },
   "source": [
    "## Web Scraping Ethics and Best Practices\n",
    "\n",
    "Before we start, it's important to understand some ethical guidelines:\n",
    "\n",
    "1. **Check robots.txt**: Always check if scraping is allowed (`website.com/robots.txt`).\n",
    "2. **Respect rate limits**: Don't overload servers with too many requests.\n",
    "3. **Identify yourself**: Use proper headers with your contact info.\n",
    "4. **Only take what you need**: Don't scrape unnecessary data.\n",
    "\n",
    "For this tutorial, we'll use websites that allow scraping for educational purposes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qk38u2BuOugY"
   },
   "source": [
    "## Downloading a Web Page\n",
    "\n",
    "Let's start by downloading a simple web page. We'll use a book catalog website that's designed for scraping practice.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1P0Dee3LOugY"
   },
   "outputs": [],
   "source": [
    "# TODO: Define your headers to identify yourself\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Educational purpose scraper)',\n",
    "    'Accept': 'text/html,application/xhtml+xml'\n",
    "}\n",
    "\n",
    "# URL of the page we want to scrape\n",
    "url = 'http://books.toscrape.com/'\n",
    "\n",
    "# TODO: Make a request to the website\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    print(f\"Successfully downloaded the page! Content length: {len(response.text)} characters\")\n",
    "else:\n",
    "    print(f\"Failed to download the page. Status code: {response.status_code}\")\n",
    "\n",
    "# Let's see what the raw HTML looks like (just the first 500 characters)\n",
    "print(response.text[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gNE3xZGsOugZ"
   },
   "source": [
    "## Parsing HTML with BeautifulSoup\n",
    "\n",
    "Now that we have the HTML content, let's parse it to extract structured information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tnnFxUTwOugZ"
   },
   "outputs": [],
   "source": [
    "# TODO: Create a BeautifulSoup object\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Let's check the title of the page\n",
    "page_title = soup.title.text\n",
    "print(f\"Page title: {page_title}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YXlIq4ykOugZ"
   },
   "source": [
    "## Extracting Book Information\n",
    "\n",
    "Let's extract information about the books shown on the page. We'll look for:\n",
    "- Book titles\n",
    "- Prices\n",
    "- Star ratings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xFUnR100OugZ"
   },
   "outputs": [],
   "source": [
    "# Find all book containers on the page\n",
    "book_containers = soup.find_all('article', class_='product_pod')\n",
    "print(f\"Found {len(book_containers)} books on this page\")\n",
    "\n",
    "# Lists to store our data\n",
    "titles = []\n",
    "prices = []\n",
    "ratings = []\n",
    "\n",
    "# TODO: Extract information from each book container\n",
    "for book in book_containers:\n",
    "    # Extract title\n",
    "    title = book.h3.a['title']\n",
    "    titles.append(title)\n",
    "\n",
    "    # Extract price\n",
    "    price = book.find('p', class_='price_color').text\n",
    "    prices.append(price)\n",
    "\n",
    "    # Extract star rating (contained in the class name)\n",
    "    star_rating = book.find('p', class_='star-rating')['class'][1]\n",
    "    ratings.append(star_rating)\n",
    "\n",
    "# Display the first 5 items from each list\n",
    "for i in range(5):\n",
    "    print(f\"Book {i+1}: {titles[i]}, Price: {prices[i]}, Rating: {ratings[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aEKVRIt1Ouga"
   },
   "source": [
    "## Converting to a DataFrame\n",
    "\n",
    "Now let's organize our scraped data into a pandas DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AWU5Z2O8Ouga"
   },
   "outputs": [],
   "source": [
    "# TODO: Create a DataFrame from the scraped data\n",
    "books_data = {\n",
    "    'Title': titles,\n",
    "    'Price': prices,\n",
    "    'Rating': ratings\n",
    "}\n",
    "\n",
    "books_df = pd.DataFrame(books_data)\n",
    "\n",
    "# Show the DataFrame\n",
    "books_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0b57K2I3Ouga"
   },
   "source": [
    "## Cleaning and Processing Data\n",
    "\n",
    "Let's clean up our data to make it more useful:\n",
    "- Convert prices from string to numeric values\n",
    "- Convert ratings to a numeric scale (e.g., 1-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8_bLxaYBOuga"
   },
   "outputs": [],
   "source": [
    "# TODO: Clean the price column (remove Â£ symbol and convert to float)\n",
    "books_df['Price'] = books_df['Price'].str.replace('Â£', '').astype(float)\n",
    "\n",
    "# TODO: Convert ratings to numeric values\n",
    "rating_mapping = {\n",
    "    'One': 1,\n",
    "    'Two': 2,\n",
    "    'Three': 3,\n",
    "    'Four': 4,\n",
    "    'Five': 5\n",
    "}\n",
    "books_df['Rating'] = books_df['Rating'].map(rating_mapping)\n",
    "\n",
    "# Show the updated DataFrame\n",
    "books_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9m-jwmkUOuga"
   },
   "source": [
    "## Analyzing the Scraped Data\n",
    "\n",
    "Now let's use pandas to analyze the data we've collected.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uBHz0KoUOugb"
   },
   "outputs": [],
   "source": [
    "# Display basic statistics\n",
    "books_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "grE7KJsLOugb"
   },
   "outputs": [],
   "source": [
    "# TODO: Calculate average price by rating\n",
    "avg_price_by_rating = books_df.groupby('Rating')['Price'].mean().sort_index()\n",
    "print(\"Average price by rating:\")\n",
    "avg_price_by_rating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1G7IYItHOugb"
   },
   "source": [
    "## ðŸ§  Challenge: Scrape Multiple Pages\n",
    "\n",
    "The book catalog has multiple pages. Can you expand your scraping to collect books from the first 3 pages?\n",
    "\n",
    "(Hint: Look at the URL pattern for pagination and use a loop)\n",
    "\n",
    "Try writing your own code below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VOgrNTy5Ougb"
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# Hint: The pagination URLs follow this pattern: 'http://books.toscrape.com/catalogue/page-{page_num}.html'\n",
    "\n",
    "all_titles = []\n",
    "all_prices = []\n",
    "all_ratings = []\n",
    "\n",
    "# Loop through multiple pages\n",
    "for page_num in range(1, 4):  # Pages 1, 2, and 3\n",
    "    # Construct the URL for each page\n",
    "    if page_num == 1:\n",
    "        page_url = 'http://books.toscrape.com/'\n",
    "    else:\n",
    "        page_url = f'http://books.toscrape.com/catalogue/page-{page_num}.html'\n",
    "\n",
    "    # Add a small delay to be polite\n",
    "    time.sleep(1)\n",
    "\n",
    "    # Make the request\n",
    "    response = requests.get(page_url, headers=headers)\n",
    "\n",
    "    # Check if successful\n",
    "    if response.status_code == 200:\n",
    "        print(f\"Successfully scraped page {page_num}\")\n",
    "\n",
    "        # Parse the page\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Extract book information\n",
    "        # (Your extraction code here)\n",
    "    else:\n",
    "        print(f\"Failed to scrape page {page_num}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YtddpcKXOugb"
   },
   "source": [
    "## Saving the Scraped Data\n",
    "\n",
    "Let's save our scraped data to a CSV file for future use.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JOvecAM3Ougb"
   },
   "outputs": [],
   "source": [
    "# TODO: Save the DataFrame to a CSV file\n",
    "books_df.to_csv('scraped_books.csv', index=False)\n",
    "print(\"Data saved to 'scraped_books.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AsZ2ny2HOugb"
   },
   "source": [
    "## Reflection\n",
    "\n",
    "- What was most interesting about the web scraping process?\n",
    "- What challenges did you encounter?\n",
    "- How could you extend this scraper to collect more detailed information?\n",
    "- How might you use web scraping in your own projects?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vz9EdmGWOugb"
   },
   "source": [
    "## Additional Resources\n",
    "\n",
    "- BeautifulSoup Documentation: https://www.crummy.com/software/BeautifulSoup/bs4/doc/\n",
    "- Requests Library: https://docs.python-requests.org/\n",
    "- Pandas Documentation: https://pandas.pydata.org/docs/\n",
    "- Web Scraping Ethics: https://www.scrapehero.com/how-to-prevent-getting-blacklisted-while-scraping/\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "colab": {
   "provenance": [],
   "include_colab_link": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
