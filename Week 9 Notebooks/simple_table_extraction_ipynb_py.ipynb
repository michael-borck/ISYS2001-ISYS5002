{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/michael-borck/ISYS2001-ISYS5002/blob/main/Week%208%20Notebooks/simple_table_extraction_ipynb_py.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_IPaHY3IRKT1"
   },
   "source": [
    "# Extracting Web Tables with Pandas: A Simple Introduction\n",
    "**Learn how to easily collect table data from websites using Python and Pandas**\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand how to extract HTML tables directly with pandas\n",
    "- Process and clean the extracted table data\n",
    "- Export the results for further analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PvsIxJtARKT2"
   },
   "source": [
    "## Introduction to Web Tables\n",
    "\n",
    "Many websites present data in HTML tables, which are perfect for analysis. Examples include:\n",
    "- Wikipedia tables of statistics\n",
    "- Sport results and rankings\n",
    "- Financial data tables\n",
    "- Government data listings\n",
    "\n",
    "The great news is that pandas makes it incredibly easy to extract these tables!\n",
    "\n",
    "### Installing and Importing\n",
    "\n",
    "We need pandas and an additional library called `lxml` for HTML parsing:\n",
    "\n",
    "```python\n",
    "!pip install pandas lxml\n",
    "```\n",
    "\n",
    "Let's import what we need:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dEx3gIJDRKT3"
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QKnkPP1gRKT3"
   },
   "source": [
    "## Using pandas.read_html()\n",
    "\n",
    "The `read_html()` function in pandas is amazing - it can automatically find and extract HTML tables from a web page. Let's try it on a simple example: a Wikipedia page with country population data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yJOYvI04RKT3"
   },
   "outputs": [],
   "source": [
    "# URL of a Wikipedia page with tables\n",
    "url = 'https://en.wikipedia.org/wiki/List_of_countries_by_population_(United_Nations)'\n",
    "\n",
    "# TODO: Extract all tables from the web page\n",
    "tables = pd.read_html(url)\n",
    "\n",
    "# Check how many tables were found\n",
    "print(f\"Number of tables found on the page: {len(tables)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hVSJMI2XRKT3"
   },
   "source": [
    "## Exploring the Extracted Tables\n",
    "\n",
    "The `read_html()` function returns a list of all tables found on the page. Let's look at what we've extracted:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2bv_5j7fRKT4"
   },
   "outputs": [],
   "source": [
    "# Let's look at the first table (index 0)\n",
    "first_table = tables[0]\n",
    "\n",
    "# Show the first few rows\n",
    "first_table.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xxAPbDmTRKT4"
   },
   "source": [
    "## Selecting the Right Table\n",
    "\n",
    "A webpage might have multiple tables. Let's find the main data table we're interested in by examining each one.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g5uVRQIbRKT4"
   },
   "outputs": [],
   "source": [
    "# TODO: Loop through all tables and print their shape (rows x columns)\n",
    "for i, table in enumerate(tables):\n",
    "    print(f\"Table {i}: Shape = {table.shape} (rows Ã— columns)\")\n",
    "    # Optional: display a small preview of each table\n",
    "    print(f\"Preview of columns: {table.columns.tolist()[:3]}...\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dO9K8a9nRKT4"
   },
   "source": [
    "After examining the tables, let's select the one with country population data. Usually, the main data table is one of the larger ones.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r-Hz-JqiRKT4"
   },
   "outputs": [],
   "source": [
    "# TODO: Select the table with population data (adjust index as needed)\n",
    "# For Wikipedia country population, it's likely to be index 1\n",
    "population_table = tables[1]  # Adjust this index based on your examination above\n",
    "\n",
    "# Show more rows to verify this is the right table\n",
    "population_table.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fskE0iqJRKT4"
   },
   "source": [
    "## Cleaning the Data\n",
    "\n",
    "Now that we have our table, let's clean it up:\n",
    "- Rename columns if needed\n",
    "- Remove unnecessary columns\n",
    "- Convert data types\n",
    "- Handle missing values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "umVQjlh0RKT4"
   },
   "outputs": [],
   "source": [
    "# Make a copy to avoid changing the original\n",
    "df = population_table.copy()\n",
    "\n",
    "# TODO: Clean up column names (remove any numeric indices if present)\n",
    "# This step depends on the actual structure of your table\n",
    "# df.columns = ['Rank', 'Country', 'Population', 'Percentage', 'Date', 'Source']\n",
    "\n",
    "# TODO: Convert population to numeric (removing commas and other characters)\n",
    "# Note: Your column name might be different based on the actual table extracted\n",
    "population_column = 'Population'  # Change this to match your actual column name\n",
    "if population_column in df.columns:\n",
    "    df[population_column] = df[population_column].astype(str).str.replace(',', '').str.extract('(\\d+)').astype(float)\n",
    "\n",
    "# Show the cleaned table\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4EKL5GZ4RKT4"
   },
   "source": [
    "## Analyzing the Data\n",
    "\n",
    "Now let's do some basic analysis with our clean data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r_pPylimRKT4"
   },
   "outputs": [],
   "source": [
    "# Get basic statistics\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "npl_-PgjRKT5"
   },
   "outputs": [],
   "source": [
    "# TODO: Find the top 10 countries by population\n",
    "top_10 = df.head(10)\n",
    "print(\"Top 10 countries by population:\")\n",
    "top_10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y_Lnlzh5RKT5"
   },
   "source": [
    "## Saving the Results\n",
    "\n",
    "Let's save our clean data to a CSV file for future use:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0VW_4_ZuRKT5"
   },
   "outputs": [],
   "source": [
    "# TODO: Save the DataFrame to a CSV file\n",
    "df.to_csv('country_populations.csv', index=False)\n",
    "print(\"Data saved to 'country_populations.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gmI-mBUXRKT5"
   },
   "source": [
    "## ðŸ§  Challenge: Extract a Different Table\n",
    "\n",
    "Try to extract a table from another Wikipedia page, such as:\n",
    "- List of countries by GDP\n",
    "- List of Nobel Prize winners\n",
    "- World's tallest buildings\n",
    "- Olympic medal counts\n",
    "\n",
    "Choose a page with tables and apply what you've learned!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AKPSfTlwRKT5"
   },
   "outputs": [],
   "source": [
    "# Your code here for the challenge\n",
    "# new_url = 'https://en.wikipedia.org/wiki/...'\n",
    "# new_tables = pd.read_html(new_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cU6WGUJ5RKT5"
   },
   "source": [
    "## More Table Extraction Examples\n",
    "\n",
    "Let's try one more example - extracting a table from a different source.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uvZ4j_ciRKT5"
   },
   "outputs": [],
   "source": [
    "# Example: Extract S&P 500 table from Wikipedia\n",
    "sp500_url = 'https://en.wikipedia.org/wiki/List_of_S%26P_500_companies'\n",
    "\n",
    "# TODO: Extract the tables\n",
    "sp500_tables = pd.read_html(sp500_url)\n",
    "\n",
    "# Check how many tables were found\n",
    "print(f\"Number of tables found: {len(sp500_tables)}\")\n",
    "\n",
    "# Get the S&P 500 companies table (likely the first table)\n",
    "sp500_companies = sp500_tables[0]\n",
    "\n",
    "# Display the first few rows\n",
    "sp500_companies.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iZ4wOFJXRKT5"
   },
   "source": [
    "## Filtering and Sorting Table Data\n",
    "\n",
    "Now that we have the S&P 500 companies table, let's do some filtering and sorting:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vj4iPVrIRKT5"
   },
   "outputs": [],
   "source": [
    "# TODO: Show just the technology sector companies\n",
    "# Note: The actual column name might be different\n",
    "sector_column = 'GICS Sector'  # Adjust based on actual column name\n",
    "if sector_column in sp500_companies.columns:\n",
    "    tech_companies = sp500_companies[sp500_companies[sector_column] == 'Information Technology']\n",
    "    print(f\"Number of technology companies: {len(tech_companies)}\")\n",
    "    tech_companies.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2I0ps2zDRKT5"
   },
   "source": [
    "## Reflection\n",
    "\n",
    "- How easy was it to extract tables compared to other web scraping methods?\n",
    "- What challenges did you encounter when working with the extracted tables?\n",
    "- What other websites with tables might be interesting to analyze?\n",
    "- How could you improve this extraction process?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kVlw2XCYRKT5"
   },
   "source": [
    "## Additional Resources\n",
    "\n",
    "- Pandas read_html documentation: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_html.html\n",
    "- More about web scraping ethics: https://www.scrapehero.com/how-to-prevent-getting-blacklisted-while-scraping/\n",
    "- Sources of tabular data: Wikipedia, government sites, sports statistics sites\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "colab": {
   "provenance": [],
   "include_colab_link": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
